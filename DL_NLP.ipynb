{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d82a92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neattext.functions as nfx\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity,linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a056de58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neattext in c:\\users\\dinesh_pc\\anaconda3\\lib\\site-packages (0.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99bd9660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Id</th>\n",
       "      <th>Image ID</th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Key Words</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Paper Type</th>\n",
       "      <th>Summarization</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multi-document Summarization via Deep Learning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multi-document summarization (MDS) is an effec...</td>\n",
       "      <td>In this article, we have presented the first c...</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>This article presents a systematic overview of...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NLP based Machine Learning Approaches for Text...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Due to the plethora of data available today, t...</td>\n",
       "      <td>We have seen that due to abundant availability...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>The article discusses the importance of text s...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abstractive text summarization using LSTM-CNN ...</td>\n",
       "      <td>Text mining . Abstractive text summarization ....</td>\n",
       "      <td>Abstractive Text Summarization (ATS), which i...</td>\n",
       "      <td>In this paper, we develop a novel LSTM-CNN bas...</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>The article presents a new framework for abstr...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEXPERTS: Decoding-Time Controlled Text Genera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Despite recent advances in natural language\\ng...</td>\n",
       "      <td>We present DEXPERTS, a method for controlled\\n...</td>\n",
       "      <td>Text generation</td>\n",
       "      <td>The paper proposes a method called DEXPERTS fo...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Survey of Knowledge-enhanced Text Generation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The goal of text-to-text generation is to make...</td>\n",
       "      <td>In this survey, we present a comprehensive rev...</td>\n",
       "      <td>Text generation</td>\n",
       "      <td>The paper discusses the challenges in text-to-...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Paper Id  Image ID                                        Paper Title  \\\n",
       "0       1.0       NaN  Multi-document Summarization via Deep Learning...   \n",
       "1       2.0       NaN  NLP based Machine Learning Approaches for Text...   \n",
       "2       3.0       NaN  Abstractive text summarization using LSTM-CNN ...   \n",
       "3       4.0       NaN  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
       "4       5.0       NaN     A Survey of Knowledge-enhanced Text Generation   \n",
       "\n",
       "                                           Key Words  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Text mining . Abstractive text summarization ....   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Multi-document summarization (MDS) is an effec...   \n",
       "1  Due to the plethora of data available today, t...   \n",
       "2   Abstractive Text Summarization (ATS), which i...   \n",
       "3  Despite recent advances in natural language\\ng...   \n",
       "4  The goal of text-to-text generation is to make...   \n",
       "\n",
       "                                          Conclusion  \\\n",
       "0  In this article, we have presented the first c...   \n",
       "1  We have seen that due to abundant availability...   \n",
       "2  In this paper, we develop a novel LSTM-CNN bas...   \n",
       "3  We present DEXPERTS, a method for controlled\\n...   \n",
       "4  In this survey, we present a comprehensive rev...   \n",
       "\n",
       "                    Paper Type  \\\n",
       "0           Text summarization   \n",
       "1  Natural Language Processing   \n",
       "2           Text summarization   \n",
       "3              Text generation   \n",
       "4              Text generation   \n",
       "\n",
       "                                       Summarization  \\\n",
       "0  This article presents a systematic overview of...   \n",
       "1  The article discusses the importance of text s...   \n",
       "2  The article presents a new framework for abstr...   \n",
       "3  The paper proposes a method called DEXPERTS fo...   \n",
       "4  The paper discusses the challenges in text-to-...   \n",
       "\n",
       "                         Topic  \n",
       "0  Natural Language Processing  \n",
       "1  Natural Language Processing  \n",
       "2  Natural Language Processing  \n",
       "3  Natural Language Processing  \n",
       "4  Natural Language Processing  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DATA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bfceaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all the methods present in the neattext function\n",
    "\n",
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44d4f0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Id</th>\n",
       "      <th>Image ID</th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Key Words</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Paper Type</th>\n",
       "      <th>Summarization</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>320.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Prediction of Terrorist Activities by Using Un...</td>\n",
       "      <td>Unsupervised learning; Distance Based Clusteri...</td>\n",
       "      <td>Terrorism now considered as a major threat to ...</td>\n",
       "      <td>Terrorist attacks prediction by using unsuperv...</td>\n",
       "      <td>sentiment analysis</td>\n",
       "      <td>The paper discusses the increasing threat of t...</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>321.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Like It or Not: A Survey of Twitter Sentiment ...</td>\n",
       "      <td>: Sentiment analysis, opinion mining, microblo...</td>\n",
       "      <td>Sentiment analysis in Twitter is a field that ...</td>\n",
       "      <td>Recent years have witnessed an increasing rese...</td>\n",
       "      <td>sentiment analysis</td>\n",
       "      <td>The text discusses the recent research interes...</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>322.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Long Short-term Memory Network over Rhetorical...</td>\n",
       "      <td>: LSTM, Rhetorical Structure Theory, sentiment...</td>\n",
       "      <td>Using deep learning models to solve sentiment ...</td>\n",
       "      <td>During learning process of text features, furt...</td>\n",
       "      <td>sentiment analysis</td>\n",
       "      <td>The text discusses the challenges of using dee...</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>323.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Combine HowNet lexicon to train phrase recursi...</td>\n",
       "      <td>Sentiment analysis Recursive autoencoder HowNe...</td>\n",
       "      <td>Detecting sentiment of sentences in online rev...</td>\n",
       "      <td>In this paper, we propose a novel method that ...</td>\n",
       "      <td>sentiment analysis</td>\n",
       "      <td>The text discusses the challenges of detecting...</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>324.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New term weighting schemes with combination of...</td>\n",
       "      <td>Sentiment classification Openion mining Term w...</td>\n",
       "      <td>The rapid growth of social media on the Web, s...</td>\n",
       "      <td>In this paper, different approaches for automa...</td>\n",
       "      <td>sentiment analysis</td>\n",
       "      <td>The article discusses the growth of sentiment ...</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>326.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sentiment Analysis in Social Media and Its App...</td>\n",
       "      <td>Sentiment analysis; Big data; Social media</td>\n",
       "      <td>Twitter and sentiment analysis application can...</td>\n",
       "      <td>The conducted systematic literature review pro...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "      <td>The article discusses how sentiment analysis i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>327.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expert Systems With Applications</td>\n",
       "      <td>Sentiment analysis \\r\\nSocial media \\r\\nTwitte...</td>\n",
       "      <td>Sentiment analysis has proven to be a valuable...</td>\n",
       "      <td>Regarding technologies, it should be noted tha...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "      <td>The article discusses the importance of sentim...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>328.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sentiment Analysis for Social Media</td>\n",
       "      <td>Sentiment Analysis, Natural Language Processin...</td>\n",
       "      <td>Sentiment analysis, the automated extraction o...</td>\n",
       "      <td>using the sentiment scores for sentiments rega...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "      <td>Sentiment analysis is the automated extraction...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>329.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A review on sentiment analysis and emotion det...</td>\n",
       "      <td>Afective computing · Natural language processi...</td>\n",
       "      <td>Social networking platforms have become an ess...</td>\n",
       "      <td>In this paper, a review of the existing techni...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "      <td>Social networking platforms have become a popu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>330.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sentiment Analysis of Twitter Data</td>\n",
       "      <td>Sentiment analysis, social media, Twitter, tweets</td>\n",
       "      <td>Nowadays, people from all around the world use...</td>\n",
       "      <td>Sentiment analysis is a field of study for ana...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "      <td>This paper presents a model for sentiment anal...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Paper Id  Image ID                                        Paper Title  \\\n",
       "323     320.0       NaN  Prediction of Terrorist Activities by Using Un...   \n",
       "324     321.0       NaN  Like It or Not: A Survey of Twitter Sentiment ...   \n",
       "325     322.0       NaN  Long Short-term Memory Network over Rhetorical...   \n",
       "326     323.0       NaN  Combine HowNet lexicon to train phrase recursi...   \n",
       "327     324.0       NaN  New term weighting schemes with combination of...   \n",
       "329     326.0       NaN  Sentiment Analysis in Social Media and Its App...   \n",
       "330     327.0       NaN                  Expert Systems With Applications    \n",
       "331     328.0       NaN                Sentiment Analysis for Social Media   \n",
       "332     329.0       NaN  A review on sentiment analysis and emotion det...   \n",
       "333     330.0       NaN                 Sentiment Analysis of Twitter Data   \n",
       "\n",
       "                                             Key Words  \\\n",
       "323  Unsupervised learning; Distance Based Clusteri...   \n",
       "324  : Sentiment analysis, opinion mining, microblo...   \n",
       "325  : LSTM, Rhetorical Structure Theory, sentiment...   \n",
       "326  Sentiment analysis Recursive autoencoder HowNe...   \n",
       "327  Sentiment classification Openion mining Term w...   \n",
       "329         Sentiment analysis; Big data; Social media   \n",
       "330  Sentiment analysis \\r\\nSocial media \\r\\nTwitte...   \n",
       "331  Sentiment Analysis, Natural Language Processin...   \n",
       "332  Afective computing · Natural language processi...   \n",
       "333  Sentiment analysis, social media, Twitter, tweets   \n",
       "\n",
       "                                              Abstract  \\\n",
       "323  Terrorism now considered as a major threat to ...   \n",
       "324  Sentiment analysis in Twitter is a field that ...   \n",
       "325  Using deep learning models to solve sentiment ...   \n",
       "326  Detecting sentiment of sentences in online rev...   \n",
       "327  The rapid growth of social media on the Web, s...   \n",
       "329  Twitter and sentiment analysis application can...   \n",
       "330  Sentiment analysis has proven to be a valuable...   \n",
       "331  Sentiment analysis, the automated extraction o...   \n",
       "332  Social networking platforms have become an ess...   \n",
       "333  Nowadays, people from all around the world use...   \n",
       "\n",
       "                                            Conclusion  \\\n",
       "323  Terrorist attacks prediction by using unsuperv...   \n",
       "324  Recent years have witnessed an increasing rese...   \n",
       "325  During learning process of text features, furt...   \n",
       "326  In this paper, we propose a novel method that ...   \n",
       "327  In this paper, different approaches for automa...   \n",
       "329  The conducted systematic literature review pro...   \n",
       "330  Regarding technologies, it should be noted tha...   \n",
       "331  using the sentiment scores for sentiments rega...   \n",
       "332  In this paper, a review of the existing techni...   \n",
       "333  Sentiment analysis is a field of study for ana...   \n",
       "\n",
       "                             Paper Type  \\\n",
       "323                  sentiment analysis   \n",
       "324                  sentiment analysis   \n",
       "325                  sentiment analysis   \n",
       "326                  sentiment analysis   \n",
       "327                  sentiment analysis   \n",
       "329  Deep Learning and Machine Learning   \n",
       "330  Deep Learning and Machine Learning   \n",
       "331  Deep Learning and Machine Learning   \n",
       "332  Deep Learning and Machine Learning   \n",
       "333  Deep Learning and Machine Learning   \n",
       "\n",
       "                                         Summarization               Topic  \n",
       "323  The paper discusses the increasing threat of t...  Sentiment Analysis  \n",
       "324  The text discusses the recent research interes...  Sentiment Analysis  \n",
       "325  The text discusses the challenges of using dee...  Sentiment Analysis  \n",
       "326  The text discusses the challenges of detecting...  Sentiment Analysis  \n",
       "327  The article discusses the growth of sentiment ...  Sentiment Analysis  \n",
       "329  The article discusses how sentiment analysis i...                 NaN  \n",
       "330  The article discusses the importance of sentim...                 NaN  \n",
       "331  Sentiment analysis is the automated extraction...                 NaN  \n",
       "332  Social networking platforms have become a popu...                 NaN  \n",
       "333  This paper presents a model for sentiment anal...                 NaN  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=[\"Key Words\",\"Abstract\"],inplace=True)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9abd45f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Key Words</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Summarization</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstractive text summarization using LSTM-CNN ...</td>\n",
       "      <td>Text mining . Abstractive text summarization ....</td>\n",
       "      <td>Abstractive Text Summarization (ATS), which i...</td>\n",
       "      <td>In this paper, we develop a novel LSTM-CNN bas...</td>\n",
       "      <td>The article presents a new framework for abstr...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A DATA MINING APPROACH FOR PREDICTION OF HEART...</td>\n",
       "      <td>Backpropagation, Data mining, Heart disease, M...</td>\n",
       "      <td>Heart disease diagnosis is a complex task whic...</td>\n",
       "      <td>In this research paper, we have presented Hear...</td>\n",
       "      <td>This research paper presents a Heart Disease P...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SOCIAL MEDIA  SENTIMENT \\nANALYSIS BASED ON CO...</td>\n",
       "      <td>NLP,RNN,\\nsentiment \\nanalysis,\\nsocial media,...</td>\n",
       "      <td>In today's world, the social media is everywhe...</td>\n",
       "      <td>In this work, we use a Recurrent Neural Networ...</td>\n",
       "      <td>The article discusses the use of natural langu...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A review of the affects of worry and generaliz...</td>\n",
       "      <td>anxiety disorders; heart diseases; worry; depr...</td>\n",
       "      <td>The aims of this review article are to present...</td>\n",
       "      <td>A growing literature highlights the associatio...</td>\n",
       "      <td>This review article examines the relationship ...</td>\n",
       "      <td>Medical Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bedside Screening to Detect Oropharyngeal Dysp...</td>\n",
       "      <td>Bedside screening , Videofluoroscopy , Fiberop...</td>\n",
       "      <td>Oropharyngeal dysphagia is a highly prevalent ...</td>\n",
       "      <td>New bedside screening tools have recently beco...</td>\n",
       "      <td>This systematic review discusses the prevalenc...</td>\n",
       "      <td>Medical Data Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Sentiment Analysis in Social Media and Its App...</td>\n",
       "      <td>Sentiment analysis; Big data; Social media</td>\n",
       "      <td>Twitter and sentiment analysis application can...</td>\n",
       "      <td>The conducted systematic literature review pro...</td>\n",
       "      <td>The article discusses how sentiment analysis i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Expert Systems With Applications</td>\n",
       "      <td>Sentiment analysis \\r\\nSocial media \\r\\nTwitte...</td>\n",
       "      <td>Sentiment analysis has proven to be a valuable...</td>\n",
       "      <td>Regarding technologies, it should be noted tha...</td>\n",
       "      <td>The article discusses the importance of sentim...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>Sentiment Analysis for Social Media</td>\n",
       "      <td>Sentiment Analysis, Natural Language Processin...</td>\n",
       "      <td>Sentiment analysis, the automated extraction o...</td>\n",
       "      <td>using the sentiment scores for sentiments rega...</td>\n",
       "      <td>Sentiment analysis is the automated extraction...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>A review on sentiment analysis and emotion det...</td>\n",
       "      <td>Afective computing · Natural language processi...</td>\n",
       "      <td>Social networking platforms have become an ess...</td>\n",
       "      <td>In this paper, a review of the existing techni...</td>\n",
       "      <td>Social networking platforms have become a popu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Sentiment Analysis of Twitter Data</td>\n",
       "      <td>Sentiment analysis, social media, Twitter, tweets</td>\n",
       "      <td>Nowadays, people from all around the world use...</td>\n",
       "      <td>Sentiment analysis is a field of study for ana...</td>\n",
       "      <td>This paper presents a model for sentiment anal...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Paper Title  \\\n",
       "2    Abstractive text summarization using LSTM-CNN ...   \n",
       "6    A DATA MINING APPROACH FOR PREDICTION OF HEART...   \n",
       "7    SOCIAL MEDIA  SENTIMENT \\nANALYSIS BASED ON CO...   \n",
       "14   A review of the affects of worry and generaliz...   \n",
       "15   Bedside Screening to Detect Oropharyngeal Dysp...   \n",
       "..                                                 ...   \n",
       "329  Sentiment Analysis in Social Media and Its App...   \n",
       "330                  Expert Systems With Applications    \n",
       "331                Sentiment Analysis for Social Media   \n",
       "332  A review on sentiment analysis and emotion det...   \n",
       "333                 Sentiment Analysis of Twitter Data   \n",
       "\n",
       "                                             Key Words  \\\n",
       "2    Text mining . Abstractive text summarization ....   \n",
       "6    Backpropagation, Data mining, Heart disease, M...   \n",
       "7    NLP,RNN,\\nsentiment \\nanalysis,\\nsocial media,...   \n",
       "14   anxiety disorders; heart diseases; worry; depr...   \n",
       "15   Bedside screening , Videofluoroscopy , Fiberop...   \n",
       "..                                                 ...   \n",
       "329         Sentiment analysis; Big data; Social media   \n",
       "330  Sentiment analysis \\r\\nSocial media \\r\\nTwitte...   \n",
       "331  Sentiment Analysis, Natural Language Processin...   \n",
       "332  Afective computing · Natural language processi...   \n",
       "333  Sentiment analysis, social media, Twitter, tweets   \n",
       "\n",
       "                                              Abstract  \\\n",
       "2     Abstractive Text Summarization (ATS), which i...   \n",
       "6    Heart disease diagnosis is a complex task whic...   \n",
       "7    In today's world, the social media is everywhe...   \n",
       "14   The aims of this review article are to present...   \n",
       "15   Oropharyngeal dysphagia is a highly prevalent ...   \n",
       "..                                                 ...   \n",
       "329  Twitter and sentiment analysis application can...   \n",
       "330  Sentiment analysis has proven to be a valuable...   \n",
       "331  Sentiment analysis, the automated extraction o...   \n",
       "332  Social networking platforms have become an ess...   \n",
       "333  Nowadays, people from all around the world use...   \n",
       "\n",
       "                                            Conclusion  \\\n",
       "2    In this paper, we develop a novel LSTM-CNN bas...   \n",
       "6    In this research paper, we have presented Hear...   \n",
       "7    In this work, we use a Recurrent Neural Networ...   \n",
       "14   A growing literature highlights the associatio...   \n",
       "15   New bedside screening tools have recently beco...   \n",
       "..                                                 ...   \n",
       "329  The conducted systematic literature review pro...   \n",
       "330  Regarding technologies, it should be noted tha...   \n",
       "331  using the sentiment scores for sentiments rega...   \n",
       "332  In this paper, a review of the existing techni...   \n",
       "333  Sentiment analysis is a field of study for ana...   \n",
       "\n",
       "                                         Summarization  \\\n",
       "2    The article presents a new framework for abstr...   \n",
       "6    This research paper presents a Heart Disease P...   \n",
       "7    The article discusses the use of natural langu...   \n",
       "14   This review article examines the relationship ...   \n",
       "15   This systematic review discusses the prevalenc...   \n",
       "..                                                 ...   \n",
       "329  The article discusses how sentiment analysis i...   \n",
       "330  The article discusses the importance of sentim...   \n",
       "331  Sentiment analysis is the automated extraction...   \n",
       "332  Social networking platforms have become a popu...   \n",
       "333  This paper presents a model for sentiment anal...   \n",
       "\n",
       "                                  Topic  \n",
       "2           Natural Language Processing  \n",
       "6    Deep Learning and Machine Learning  \n",
       "7           Natural Language Processing  \n",
       "14                Medical Data Analysis  \n",
       "15                Medical Data Analysis  \n",
       "..                                  ...  \n",
       "329                                 NaN  \n",
       "330                                 NaN  \n",
       "331                                 NaN  \n",
       "332                                 NaN  \n",
       "333                                 NaN  \n",
       "\n",
       "[292 rows x 6 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.columns[[0, 6, 1]], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b6566",
   "metadata": {},
   "source": [
    "## df['Paper Title'].iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa1e6ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     DATA MINING APPROACH PREDICTION HEART DISEASE ...\n",
       "7        SOCIAL MEDIA SENTIMENT ANALYSIS BASED COVID 19\n",
       "14    review affects worry generalized anxiety disor...\n",
       "15    Bedside Screening Detect Oropharyngeal Dysphag...\n",
       "Name: Paper Title, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Paper Title'] = df['Paper Title'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df['Paper Title'] = df['Paper Title'].apply(nfx.remove_special_characters)\n",
    "\n",
    "df['Paper Title'].iloc[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d132f91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<292x944 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2386 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvect = CountVectorizer()\n",
    "\n",
    "cv_mat = countvect.fit_transform(df['Paper Title'])\n",
    "\n",
    "cv_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cb77b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_mat.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6c784ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>01</th>\n",
       "      <th>19</th>\n",
       "      <th>19802019</th>\n",
       "      <th>2010</th>\n",
       "      <th>2019</th>\n",
       "      <th>24</th>\n",
       "      <th>307</th>\n",
       "      <th>3d</th>\n",
       "      <th>617</th>\n",
       "      <th>abstractive</th>\n",
       "      <th>...</th>\n",
       "      <th>weightbased</th>\n",
       "      <th>weighting</th>\n",
       "      <th>women</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>workers</th>\n",
       "      <th>workrelated</th>\n",
       "      <th>worry</th>\n",
       "      <th>years</th>\n",
       "      <th>yolo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 944 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     01  19  19802019  2010  2019  24  307  3d  617  abstractive  ...  \\\n",
       "0     0   0         0     0     0   0    0   0    0            1  ...   \n",
       "1     0   0         0     0     0   0    0   0    0            0  ...   \n",
       "2     0   1         0     0     0   0    0   0    0            0  ...   \n",
       "3     0   0         0     0     0   0    0   0    0            0  ...   \n",
       "4     0   0         0     0     0   0    0   0    0            0  ...   \n",
       "..   ..  ..       ...   ...   ...  ..  ...  ..  ...          ...  ...   \n",
       "287   0   0         0     0     0   0    0   0    0            0  ...   \n",
       "288   0   0         0     0     0   0    0   0    0            0  ...   \n",
       "289   0   0         0     0     0   0    0   0    0            0  ...   \n",
       "290   0   0         0     0     0   0    0   0    0            0  ...   \n",
       "291   0   0         0     0     0   0    0   0    0            0  ...   \n",
       "\n",
       "     weightbased  weighting  women  word  words  workers  workrelated  worry  \\\n",
       "0              0          0      0     0      0        0            0      0   \n",
       "1              0          0      0     0      0        0            0      0   \n",
       "2              0          0      0     0      0        0            0      0   \n",
       "3              0          0      0     0      0        0            0      1   \n",
       "4              0          0      0     0      0        0            0      0   \n",
       "..           ...        ...    ...   ...    ...      ...          ...    ...   \n",
       "287            0          0      0     0      0        0            0      0   \n",
       "288            0          0      0     0      0        0            0      0   \n",
       "289            0          0      0     0      0        0            0      0   \n",
       "290            0          0      0     0      0        0            0      0   \n",
       "291            0          0      0     0      0        0            0      0   \n",
       "\n",
       "     years  yolo  \n",
       "0        0     0  \n",
       "1        0     0  \n",
       "2        0     0  \n",
       "3        0     0  \n",
       "4        0     0  \n",
       "..     ...   ...  \n",
       "287      0     0  \n",
       "288      0     0  \n",
       "289      0     0  \n",
       "290      0     0  \n",
       "291      0     0  \n",
       "\n",
       "[292 rows x 944 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv_words = pd.DataFrame(cv_mat.todense(),columns=countvect.get_feature_names())\n",
    "\n",
    "df_cv_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d27ec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.14285714, ..., 0.        , 0.15430335,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.1767767 ],\n",
       "       [0.14285714, 0.        , 1.        , ..., 0.75592895, 0.3086067 ,\n",
       "        0.37796447],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.75592895, ..., 1.        , 0.40824829,\n",
       "        0.5       ],\n",
       "       [0.15430335, 0.        , 0.3086067 , ..., 0.40824829, 1.        ,\n",
       "        0.40824829],\n",
       "       [0.        , 0.1767767 , 0.37796447, ..., 0.5       , 0.40824829,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_mat = cosine_similarity(cv_mat)\n",
    "\n",
    "cosine_sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "771e2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates \n",
    "\n",
    "course_index = pd.Series(df.index,index = df['Paper Title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f570cf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_index\n",
    "course_dict = dict(course_index)\n",
    "\n",
    "\n",
    "course_keys = list(course_dict.keys())\n",
    "\n",
    "touse = {}\n",
    "counter = 0\n",
    "keyword = 'Java'\n",
    "for key,value in course_dict.items():\n",
    "    if keyword in key:\n",
    "        touse[counter] = key\n",
    "        \n",
    "    counter+=1\n",
    "    \n",
    "touse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "343293b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Key Words</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Summarization</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Paper Title, Key Words, Abstract, Conclusion, Summarization, Topic]\n",
       "Index: []"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[df['Paper Title'].str.contains('Python')]\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5792aef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Key Words</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Summarization</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Paper Title, Key Words, Abstract, Conclusion, Summarization, Topic]\n",
       "Index: []"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top6 = temp.sort_values(by = 'Topic',ascending=False).head(6)\n",
    "\n",
    "top6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5495c923",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'deep learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'deep learning'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DINESH~1\\AppData\\Local\\Temp/ipykernel_8388/350430286.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcourse_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deep learning'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'deep learning'"
     ]
    }
   ],
   "source": [
    "index = course_index['deep learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84eef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58260a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8efc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580ea27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1f4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca83c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
